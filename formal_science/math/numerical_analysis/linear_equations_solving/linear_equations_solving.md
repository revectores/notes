# Linear Equations Solving

$$
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\b}{\boldsymbol}
\newcommand{\bx}{\b x}
\newcommand{\by}{\b y}
\newcommand{\bb}{\b b}
\newcommand{\bg}{\b g}
\newcommand{\w}{\widetilde}
\newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\o}{\overline}
$$

We've kown that the equations with $n$-variables
$$
\left\{\begin{array}{ll}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1  \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2  \\
\cdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n  \\
\end{array}\right.
$$

can be written in matrix form $A\bx = \bb$
$$
\begin{bmatrix}
a_{11} & a_{12} &\cdots& a_{1n}  \\
a_{21} & a_{22} &\cdots& a_{2n}  \\
\vdots & \vdots &\ddots & \vdots \\
a_{n1} & a_{n2} &\cdots& a_{nn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n \\
\end{bmatrix}
$$

where $A$ is **coefficient matrix**, $\bb$ is **constant vector**, and $\bx$ is **solution vector**.

[Cramer's rule]() guranttes that for matrix $A$ with $\det A \neq 0$, the solution must exist and is unique:
$$
x_i = \frac{D_i}{D}, \quad i = 1, 2, \ldots, n
$$

where $D_i$ can be generated by replacing the $i$-th column of coefficient matrix $A$ with constant $\bb$. Although plays an important role in the thoery of linear equations, the time complexity $O(n!\times n)$ of computing determinant makes it not acceptable to apply Cramer's rule in large scale of linear systems.



##### # Gaussian Elimination

[Gaussian elimination](), instead, applies [elementary matrix transformation]() to augmented matrix $\begin{bmatrix}A & b\end{bmatrix}^T$ which aims to convert the matrix into an upper triangular matrix, after this transformation, the equation can be easily solved by substituting (down to top). The time complexity for this approach is $O(n^3)$.

The Gaussian elimination requires
$$
\Delta_k =
\begin{vmatrix}
a_{11} & a_{12} &\cdots& a_{1k}  \\
a_{21} & a_{22} &\cdots& a_{2k}  \\
\vdots & \vdots &\ddots & \vdots \\
a_{k1} & a_{k2} &\cdots& a_{kk}
\end{vmatrix}
\neq 0
$$
for any $k<n$. While the solution exists only requires $\det A \neq 0$. That is, Gaussian elimination can not be applied to some solvable functions with $a_{kk}^{(k-1)} = 0$ during the elimination. Besides, the round-off error will increase for small $a_{kk}^{(k-1)}$.



##### # Gaussian Elimination with Column Pivoting

To overcome the flaw of Gaussian elimination, the column pivoting mechanism for each iteration in elimination is proposed. Formally, for each $k = 1, 2, \ldots, n-1$, we find the element with maximal absolute value $|a_{m,k}^{(k-1)}|$ within $|a_{k,k}^{k-1}|, |a_{k+1,k}^{k-1}|, \ldots |a_{n,k}^{k-1}|$ then swap row $k, m$ before elimination. With column pivoting applied, we can solve any linear equations with $\det A \neq 0$. Since the total time complexity of comparison process is $O(n^2)$, the time complexity Gaussian elimination with column pivoting is still $O(n^3)$.



##### # Gauss-Jordan Elimination

Gaussian elimination execute substitution process right after the matrix is converted to upper triangular matrix, while we can do further elimination to convert it into a diagonal, which is called **Gauss-Jordan elimination**.







### 2. Matrix Decomposition

##### # Linear Transformation and Matrix Decomposition

An elementary line transformation is equivalent to multiply with a elementary matrix. The process of Gaussian elimination is equivalent to multiple a matrix $T$ to $A$, results in a upper triangular matrix:
$$
TA = U
$$

Multiple $L = T^{-1}$ on both sides:
$$
A = LU
$$
where
$$
L = T^{-1} =
\begin{bmatrix}
1 \\
l_{21} & 1  \\
\vdots & \vdots &\ddots \\
l_{n1} & l_{n2} & \cdots & 1
\end{bmatrix}
$$
That is, we devide the matrix $A$ into the multiplication of two matrixes $L$, $U$. where $L$ and $U$ are lower and upper triangular matrix respectively. Equivalently, we can also decompose by $A=UL$.

For form $A=LU$, if $L$ is contrainted as the unit lower triangle matrix, this is called **Doolittle decomposition**, while for form $A = LU$, and the $U$ is contrainted as the unit upper triangle matrix, 

This composition process is not relavent to $\bb$, hence if there are multiple equations with same $A$ but different $\bb$, the composition method reduce the time complexity by cancelling the repetative processing of $A$.


$$
A = \begin{bmatrix}
a_{11} & a_{12} &\cdots& a_{1n}  \\
a_{21} & a_{22} &\cdots& a_{2n}  \\
\vdots & \vdots &\ddots & \vdots \\
a_{n1} & a_{n2} &\cdots& a_{nn}
\end{bmatrix}
=
\begin{bmatrix}
1 \\
l_{21} & 1  \\
\vdots & \vdots &\ddots \\
l_{n1} & l_{n2} & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
u_{11} & u_{12} &\cdots & u_{1n}  \\
       & u_{22} &\cdots & u_{2n}  \\
	   &        &\ddots & \vdots \\
       &        &       & u_{nn}
\end{bmatrix}
$$

$$
a_{11} = \sum_{r=1}^n l_{1r}u_{r1} =
\begin{bmatrix}
1 & 0 & \cdots & 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{11} \\
0 \\
\vdots \\
0
\end{bmatrix}
= \sum_{r=1}^1 l_{1r}u_{r1} = u_{11}
$$

$$
a_{1j} = \sum_{r=1}^n l_{1r}u_{rj} =
\begin{bmatrix}
1 & 0 & \cdots & 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{1j} \\
u_{2j} \\
\vdots \\
0
\end{bmatrix}
= \sum_{r=1}^1 l_{1r}u_{rj} = u_{1j}
$$
Hence $a_{ij} = u_{1j}$, $j=1,2, \dots, n$.


$$
\begin{align}
a_{kj}
&= \sum_{r=1}^n l_{kr}u_{rj} =
\begin{bmatrix}
l_{k1} & l_{k2} & \cdots & l_{k,k-1} & 1 & 0 & \cdots & 0 \\
\end{bmatrix}
\begin{bmatrix}
u_{1j} \\
\vdots \\
u_{jj} \\
0 \\
\vdots \\
0
\end{bmatrix} \\
&= \sum_{r=1}^n l_{kr}u_{rj}
= \sum_{r=1}^{k-1} l_{kr}u_{rj} + u_{kj}
\end{align}
$$




For Crout decomposition,







##### # Decomposition of Positiv Definite Matrix

For the positive definitive matrix, there exists lower triangle matrix $U$ which satisifies $A = UU^T$, this is called the square root method, since this computation process involves the computation about square root, in practice we often decompose as form $A = LDL^T$ instaed.



We first apply Doolittle decomposition to matrix and extract the diagonal line of $U$:
$$
\begin{align}
A = LU
&= \begin{bmatrix}
1 \\
l_{21} & 1  \\
\vdots & \vdots &\ddots \\
l_{n1} & l_{n2} & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
u_{11} & u_{12} &\cdots & u_{1n}  \\
       & u_{22} &\cdots & u_{2n}  \\
	   &        &\ddots & \vdots \\
       &        &       & u_{nn}
\end{bmatrix} \\
&=
\begin{bmatrix}
1 \\
l_{21} & 1  \\
\vdots & \vdots &\ddots \\
l_{n1} & l_{n2} & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
u_{11} & 		&  		&  \\
       & u_{22} &		& \\
	   &        &\ddots & \\
       &        &       & u_{nn}
\end{bmatrix}
\begin{bmatrix}
\o u_{11} & \o u_{12} &\cdots & \o u_{1n}  \\
       & \o u_{22} &\cdots & \o u_{2n}  \\
	   &        &\ddots & \vdots \\
       &        &       & \o u_{nn}
\end{bmatrix}
\end{align}
$$
 Since $A$ is symmetric positive definite matrix, we have $u_{ii}>0$. We can prove $L = \o U$ since
$$
A = LU = LD\o U^T = A^T = \o U(DL^T)
$$
That is, 
$$
\begin{align}
A = LDL^T
= \begin{bmatrix}
1 \\
l_{21} & 1  \\
\vdots & \vdots &\ddots \\
l_{n1} & l_{n2} & \cdots & 1
\end{bmatrix}
\begin{bmatrix}
u_{11} & 		&  		&  \\
       & u_{22} &		& \\
	   &        &\ddots & \\
       &        &       & u_{nn}
\end{bmatrix}
\begin{bmatrix}
1 	   & l_{21} &\cdots & l_{n1}  \\
       & 1 		&\cdots & l_{n2}  \\
	   &        &\ddots & \vdots \\
       &        &       & 1
\end{bmatrix}
\end{align}
$$

Now we have to apply three steps:

1) Solve $LZ = B$, where $Z = DL^TX$
$$
z_i = b_i - \sum_{k=1}^{i-1}l_{ij}z_j, \quad i = 1, 2, \ldots, n
$$

$$
y_i = 
$$



> **Example**. Solve the equation
> $$
> \begin{bmatrix}
> 1 & -1 & 1 \\
> -1 & 3 & -2 \\
> 1 & -2 & 4.5
> \end{bmatrix}
> \begin{bmatrix}
> x_1 \\
> x_2 \\
> x_3
> \end{bmatrix}
> =
> \begin{bmatrix}
> 4 \\
> -8 \\
> 12
> \end{bmatrix}
> $$
> by $LDL^T$ decomposition.
>
> 



##### # Condition Number of Matrix

For the non-singular matrix $A$, we define
$$
\kappa_p(A) = \norm{A}_p \norm{A^{-1}}_p
$$
as the **condition number** of matrix $A$. Although there are multiple norm numbers 
$$
\frac{\norm {\delta x}}{\norm x} \le \frac{\kappa_A \dfrac{\norm{\delta A}}{\norm A}}{1 - \kappa_A\dfrac{\norm{\delta A}}{\norm A}}
$$
We call the matrix with large $\kappa_A$ ill-conditioned, that is, the small error causes the solution 





### Linear Equation Solving by Iteration

Transform the equations $AX = \by$ into $X = MX + \bg$, for any $X^{(0)}\in R^n$, if the iteration series $\{X^{(k)}\}$ converges, the limit of iteration series $X^*$ is the solution of equations $AX=\by$. 



If $X^*$ is the solution of equations $AX=\by$, then
$$
X^* = MX^* + g
$$

$$
\begin{align}
X^* - X^{(k+1)}
&= M(X^*-X^{k}) \\
&= M^2(X^*-X^{k-1}) \\
&= \cdots \\
&= M^{k+1}(X^*-X^{(0)})
\end{align}
$$

$\displaystyle \lim_{k\rightarrow \infin} M^{k} = 0$ if and only if $\rho(M) < 1$. Hence we define the matrix with spectral radius less than 1 **convergent matrix**.

That is, whether the linear equations converges depends on the property of iteration matrix, regardless of the solution $\alpha$ and $X^{(0)}$.

By definition, to compute the spectral radius of matrix, we have to compute the 





##### # Jacobi Iteration

$$
\left\{\begin{array}{ll}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = y_1  \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = y_2  \\
\cdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = y_n  \\
\end{array}\right.
$$


$$
\left\{\begin{array}{ll}
x_1 = \dfrac{1}{a_{11}}\left({-a_{12}x_2 - \cdots - a_{1n}x_n + y_1}\right)  \\
x_2 = \dfrac{1}{a_{22}}\left({-a_{21}x_2 - \cdots - a_{2n}x_n + y_2}\right)  \\
\cdots \\
x_n = \dfrac{1}{a_{nn}}\left({-a_{n1}x_2 - \cdots - a_{nn}x_n + y_n}\right)  \\
\end{array}\right.
$$



The iteration form
$$
\left\{\begin{array}{ll}
x_1^{(k+1)} = \dfrac{1}{a_{11}}\left({-a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)} + y_1}\right)  \\
x_2^{(k+1)} = \dfrac{1}{a_{22}}\left({-a_{21}x_2^{(k)} - \cdots - a_{2n}x_n^{(k)} + y_2}\right)  \\
\cdots \\
x_n^{(k+1)} = \dfrac{1}{a_{nn}}\left({-a_{n1}x_1^{k} - \cdots - a_{n, n-1}x_n^{(k)} + y_n}\right)  \\
\end{array}\right.
$$
Denote $b_{ij} = - a_{ij}/a_{ii}, g_{i} = y_i/a_{ii}$, the matrix form of Jacobi iteration is
$$

$$




> Solve equations with Jacobi iteration:
> $$
> \left\{\begin{array}{ll}
> 2x_1 - x_2 - x_3 = -5 \\
> x_1 + 5x_2 - x_3 = 8 \\
> x_1 + x_2 + 10x_3 = 11
> \end{array}\right.
> $$
> 
>
> 



If the matrix $A$ meets one of the condition:

- $|a_{ii}| > \displaystyle \sum_{j=1\\j\neq i}^n |a_ij|,\quad i=1, 2, \ldots, n$.
- $|a_{jj}| > \displaystyle \sum_{i=1\\i\neq j}^n |a_ij|,\quad j=1, 2, \ldots, n$

The Jacobi iteration must converge.

> **Proof**. 





##### # Gauss-Seidel Iteration

The Gauss-Seidel iteration use those new values computed in current iteration instead of last one:
$$
\left\{\begin{array}{ll}
x_1^{(k+1)} = \dfrac{1}{a_{11}}\left({-a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)} + y_1}\right)  \\
x_2^{(k+1)} = \dfrac{1}{a_{22}}\left({-a_{21}x_2^{(k+1)} - \cdots - a_{2n}x_n^{(k)} + y_2}\right)  \\
\cdots \\
x_n^{(k+1)} = \dfrac{1}{a_{nn}}\left({-a_{n1}x_1^{(k+1)} - \cdots - a_{n, n-1}x_{n-1}^{(k+1)} + y_n}\right)  \\
\end{array}\right.
$$
Denote
$$
D =
\begin{bmatrix}
a_{11}    \\
& a_{22}  \\
&& \ddots \\
&&& a_{nn}
\end{bmatrix},
L =
\begin{bmatrix}
0 \\
a_{21} & 0  \\
\vdots & \vdots &\ddots \\
a_{n1} & a_{n2} & \cdots & 0
\end{bmatrix},
U =
\begin{bmatrix}
0	   & a_{12} &\cdots & a_{1n}  \\
       & 0	    &\cdots & a_{2n}  \\
	   &        &\ddots & \vdots \\
       &        &       & 0
\end{bmatrix}
$$
We have
$$
AX = (D+L+U)X = (D+L)X + UX = \by
$$
That is,
$$
(D+L)X = -UX + \by
$$
hence
$$
X^{(k+1)} = -(D+L)^{-1}UX^{(k)} + (D+L)^{-1}\by
$$
Denote $S = -(D+L)^{-1}U, \b f = (D+L)^{-1}\by$, the Gauss-Seidel iteration can be expressed by
$$
X^{k+1} = SX^{k} + \b f
$$


### Successive Over-Relaxation


$$
\begin{align}
& X^{(k+1)} = X^{(k)} + \omega\Delta X^{(k)} \\
& X^{(k+1)} = X^{(k)} + \omega(X^{(k+1)} - X^{(k)} ) \\
& X^{(k+1)} = (1-\omega)X^{(k)} + \omega\Delta X^{(k+1)} \\
\end{align}
$$
Hence
$$
X^{(k+1)} = (1-\omega)X^{(k)} + \omega(\widetilde LX^{(k+1)} + \widetilde UX^{(k)} + \bg)
$$

$$
\left\{\begin{array}{ll}
x_1^{(k+1)} = (1-\omega)x_1^{(k)} + \omega(b_{12}x_2^{k} + \cdots + b_{1n}x_n^{(k)} + g_1) \\
x_2^{(k+1)} = (1-\omega)x_2^{(k)} + \omega(b_{21}x_2^{k} + \cdots + b_{2n}x_n^{(k)} + g_2) \\
\cdots \\
x_2^{(k+1)} = (1-\omega)x_n^{(k)} + \omega(b_{n1}x_2^{(k+1)} + \cdots + b_{n, n-1}x_{n-1}^{(k+1)} + g_n) \\
\end{array}\right.
$$

$$
\begin{align}
& X^{(k+1)} = (1-\omega)X^{(k)} + \omega(\widetilde LX^{(k+1)} + \widetilde UX^{(k)} + \bg) \\
& (I - \omega \widetilde L)X^{(k+1)} = ((1-\omega)I + \omega \widetilde U)X^{(k)} + \omega\bg \\
& X^{(k+1)} = (I-\omega)
\end{align}
$$




The necessary conditions of SOR is $0 < \omega < 2$ . 

For succeeive iteration matrix $S_\omega$, the iteration converges if $\rho(S_\omega) < 1$, or $\norm{S_\omega}_p < 1$.

If $A$ is the positive determined matrix, $0 < \omega < 2$

We call the SOR with $0 < \omega < 1$ as 





